{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Kendra Custom Connector\n",
    "\n",
    "### Create two custom connectors\n",
    "#### First connector will index AWS Glue Jobs\n",
    "#### Second connector will index PostGres tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a python environment\n",
    "\n",
    "# !conda create -y --name kendra-custom-connector python=3.11.8\n",
    "# !conda init && activate kendra-custom-connector\n",
    "# !conda install -n kendra-custom-connector ipykernel --update-deps --force-reinstall -y\n",
    "# !conda install -c conda-forge ipython-sql\n",
    "\n",
    "## OR\n",
    "# !python3 -m venv venv\n",
    "# !source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
    "\n",
    "# install ipykernel, which consists of IPython as well\n",
    "# !pip install ipykernel\n",
    "# create a kernel that can be used to run notebook commands inside the virtual environment\n",
    "# !python3 -m ipykernel install --user --name=venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Install dependencies\n",
    "\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Import necessary libraries and load environment variables\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv, set_key\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "# loading environment variables that are stored in local file\n",
    "local_env_filename = 'dev.env'\n",
    "load_dotenv(find_dotenv(local_env_filename),override=True)\n",
    "\n",
    "os.environ['REGION'] = os.getenv('REGION')\n",
    "os.environ['KENDRA_INDEX'] = os.getenv('KENDRA_INDEX')\n",
    "os.environ['KENDRA_ROLE'] = os.getenv('KENDRA_ROLE')\n",
    "os.environ['CUSTOM_DATA_SOURCE_ID_1'] = os.getenv('CUSTOM_DATA_SOURCE_ID_1')\n",
    "os.environ['CUSTOM_DATA_SOURCE_ID_2'] = os.getenv('CUSTOM_DATA_SOURCE_ID_2')\n",
    "os.environ['AMAZON_Q_APP_ID'] = os.getenv('AMAZON_Q_APP_ID')\n",
    "os.environ['Q_CUSTOM_DATA_SOURCE_ID_1'] = os.getenv('Q_CUSTOM_DATA_SOURCE_ID_1')\n",
    "os.environ['Q_CUSTOM_DATA_SOURCE_ID_2'] = os.getenv('Q_CUSTOM_DATA_SOURCE_ID_2')\n",
    "os.environ['DEMO_S3_BUCKET'] = os.getenv('DEMO_S3_BUCKET')\n",
    "os.environ['DEMO_S3_KEY'] = os.getenv('DEMO_S3_KEY')\n",
    "os.environ['CLOUDFRONT_URL'] = os.getenv('CLOUDFRONT_URL')\n",
    "\n",
    "REGION = os.environ['REGION']\n",
    "KENDRA_INDEX = os.environ['KENDRA_INDEX']\n",
    "KENDRA_ROLE = os.environ['KENDRA_ROLE']\n",
    "CUSTOM_DATA_SOURCE_ID_1 = os.environ['CUSTOM_DATA_SOURCE_ID_1']\n",
    "CUSTOM_DATA_SOURCE_ID_2 = os.environ['CUSTOM_DATA_SOURCE_ID_2']\n",
    "AMAZON_Q_APP_ID = os.environ['AMAZON_Q_APP_ID']\n",
    "Q_CUSTOM_DATA_SOURCE_ID_1 = os.environ['Q_CUSTOM_DATA_SOURCE_ID_1']\n",
    "Q_CUSTOM_DATA_SOURCE_ID_2 = os.environ['Q_CUSTOM_DATA_SOURCE_ID_2']\n",
    "DEMO_S3_BUCKET = os.environ['DEMO_S3_BUCKET']\n",
    "DEMO_S3_KEY = os.environ['DEMO_S3_KEY']\n",
    "CLOUDFRONT_URL = os.environ['CLOUDFRONT_URL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom data source in Amazon Kendra\n",
    "import boto3\n",
    "def create_custom_data_source(index_id, data_source_name, data_source_description):\n",
    "    kendra = boto3.client('kendra')\n",
    "\n",
    "    try:\n",
    "        # Create a data source\n",
    "        response = kendra.create_data_source(\n",
    "            IndexId=index_id,\n",
    "            Name=data_source_name,\n",
    "            Type=\"CUSTOM\",\n",
    "            Description=data_source_description\n",
    "\n",
    "        )\n",
    "        print(f\"Data source created with ID: {response['Id']}\")\n",
    "        return response['Id']\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating data source: {e}\")\n",
    "        raise      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the dataframe to documents for kendra ingestion\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "def parse_to_docs(dataSourceId, jobExecutionId, df):\n",
    "    documents = []\n",
    "   \n",
    "    for index_label, row_series in df.iterrows():\n",
    "        Text = df.at[index_label , 'Text']\n",
    "        Title = df.at[index_label , 'Title']\n",
    "        Url =  df.at[index_label , 'Url']\n",
    "        CrawledDate = df.at[index_label , 'CrawledDate']\n",
    "        docID =  df.at[index_label , 'docID']\n",
    "        doc = {\n",
    "            \"Id\": docID,\n",
    "            \"Blob\": Text,\n",
    "            \"Title\": Title,\n",
    "            \"Attributes\": [\n",
    "                {\n",
    "                \"Key\": \"_data_source_id\",\n",
    "                \"Value\": {\n",
    "                    \"StringValue\": dataSourceId\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                \"Key\": \"_data_source_sync_job_execution_id\",\n",
    "                \"Value\": {\n",
    "                    \"StringValue\": jobExecutionId\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                \"Key\": \"_source_uri\",\n",
    "                \"Value\": {\n",
    "                    \"StringValue\": Url\n",
    "                    }    \n",
    "                },\n",
    "                {\n",
    "                \"Key\": \"_created_at\",\n",
    "                \"Value\": {\n",
    "                    \"DateValue\": CrawledDate\n",
    "                    }    \n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        documents.append(doc)\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the data source sync job\n",
    "\n",
    "def run_data_source_sync_job(data_source_id, index_id, df):\n",
    "    kendra = boto3.client('kendra')\n",
    "\n",
    "    #Start a data source sync job\n",
    "    result = kendra.start_data_source_sync_job(\n",
    "        Id = data_source_id,\n",
    "        IndexId = index_id\n",
    "        )\n",
    "\n",
    "    print(\"Start data source sync operation: \")\n",
    "    print(result)\n",
    "\n",
    "    #Obtain the job execution ID from the result\n",
    "    job_execution_id = result['ExecutionId']\n",
    "    print(\"Job execution ID: \"+job_execution_id)\n",
    "\n",
    "    #Start ingesting documents\n",
    "    try:\n",
    "        #parse docs for ingestion\n",
    "        docs = parse_to_docs(data_source_id, job_execution_id, df)\n",
    "        #batchput docs\n",
    "        result = kendra.batch_put_document(\n",
    "            IndexId = index_id,\n",
    "            Documents = docs\n",
    "            )\n",
    "\n",
    "\n",
    "    #Stop data source sync job\n",
    "    finally:\n",
    "        #Stop data source sync\n",
    "        result = kendra.stop_data_source_sync_job(\n",
    "            Id = data_source_id,\n",
    "            IndexId = index_id\n",
    "            )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source created with ID: 7ad51f33-e31e-43f1-90aa-7d3fc83641b0\n",
      "custom_data_source_id_1: 7ad51f33-e31e-43f1-90aa-7d3fc83641b0\n"
     ]
    }
   ],
   "source": [
    "# CREATE CUSTOM DATA SOURCE 1\n",
    "if CUSTOM_DATA_SOURCE_ID_1 == \"XXX\":\n",
    "    custom_data_source_id_1 = create_custom_data_source(KENDRA_INDEX, \n",
    "                                                \"custom-Postgres\",\n",
    "                                                \"Custom data source for a PostGres database\"\n",
    "                                                )\n",
    "\n",
    "    print(f'custom_data_source_id_1: {custom_data_source_id_1}')\n",
    "    os.environ['CUSTOM_DATA_SOURCE_ID_1'] = custom_data_source_id_1\n",
    "    dotenv.set_key(local_env_filename, \"CUSTOM_DATA_SOURCE_ID_1\", os.environ[\"CUSTOM_DATA_SOURCE_ID_1\"])\n",
    "    CUSTOM_DATA_SOURCE_ID_1=os.environ[\"CUSTOM_DATA_SOURCE_ID_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Url</th>\n",
       "      <th>CrawledDate</th>\n",
       "      <th>CreatedBy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>public.categories</td>\n",
       "      <td>categories</td>\n",
       "      <td>The \"categories\" table has 4 columns: \"categor...</td>\n",
       "      <td>https://d3q8adh3y5sxpk.cloudfront.net/rag-demo...</td>\n",
       "      <td>2024-10-11T16:03:29.505270</td>\n",
       "      <td>huthmac@amazon.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>public.customer_customer_demo</td>\n",
       "      <td>customer_customer_demo</td>\n",
       "      <td>The \"customer_customer_demo\" table has two col...</td>\n",
       "      <td>https://d3q8adh3y5sxpk.cloudfront.net/rag-demo...</td>\n",
       "      <td>2024-10-11T16:03:32.083586</td>\n",
       "      <td>huthmac@amazon.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>public.customer_demographics</td>\n",
       "      <td>customer_demographics</td>\n",
       "      <td>The \"customer_demographics\" table contains inf...</td>\n",
       "      <td>https://d3q8adh3y5sxpk.cloudfront.net/rag-demo...</td>\n",
       "      <td>2024-10-11T16:03:35.020689</td>\n",
       "      <td>huthmac@amazon.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>public.customers</td>\n",
       "      <td>customers</td>\n",
       "      <td>The \"customers\" table contains information abo...</td>\n",
       "      <td>https://d3q8adh3y5sxpk.cloudfront.net/rag-demo...</td>\n",
       "      <td>2024-10-11T16:03:58.010916</td>\n",
       "      <td>huthmac@amazon.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>public.employee_territories</td>\n",
       "      <td>employee_territories</td>\n",
       "      <td>The \"employee_territories\" table has 2 columns...</td>\n",
       "      <td>https://d3q8adh3y5sxpk.cloudfront.net/rag-demo...</td>\n",
       "      <td>2024-10-11T16:04:02.856653</td>\n",
       "      <td>huthmac@amazon.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           docID                   Title  \\\n",
       "0              public.categories              categories   \n",
       "1  public.customer_customer_demo  customer_customer_demo   \n",
       "2   public.customer_demographics   customer_demographics   \n",
       "3               public.customers               customers   \n",
       "4    public.employee_territories    employee_territories   \n",
       "\n",
       "                                                Text  \\\n",
       "0  The \"categories\" table has 4 columns: \"categor...   \n",
       "1  The \"customer_customer_demo\" table has two col...   \n",
       "2  The \"customer_demographics\" table contains inf...   \n",
       "3  The \"customers\" table contains information abo...   \n",
       "4  The \"employee_territories\" table has 2 columns...   \n",
       "\n",
       "                                                 Url  \\\n",
       "0  https://d3q8adh3y5sxpk.cloudfront.net/rag-demo...   \n",
       "1  https://d3q8adh3y5sxpk.cloudfront.net/rag-demo...   \n",
       "2  https://d3q8adh3y5sxpk.cloudfront.net/rag-demo...   \n",
       "3  https://d3q8adh3y5sxpk.cloudfront.net/rag-demo...   \n",
       "4  https://d3q8adh3y5sxpk.cloudfront.net/rag-demo...   \n",
       "\n",
       "                  CrawledDate           CreatedBy  \n",
       "0  2024-10-11T16:03:29.505270  huthmac@amazon.com  \n",
       "1  2024-10-11T16:03:32.083586  huthmac@amazon.com  \n",
       "2  2024-10-11T16:03:35.020689  huthmac@amazon.com  \n",
       "3  2024-10-11T16:03:58.010916  huthmac@amazon.com  \n",
       "4  2024-10-11T16:04:02.856653  huthmac@amazon.com  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get database details\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# helper methods to run SQL queries\n",
    "\n",
    "from sqlalchemy import create_engine, MetaData, text\n",
    "import boto3\n",
    "\n",
    "def get_secret(secret_name):\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(service_name='secretsmanager', region_name=REGION)\n",
    "    get_secret_value_response = client.get_secret_value(SecretId=secret_name)\n",
    "    return get_secret_value_response\n",
    "\n",
    "def run_sql_query(statement):\n",
    "    try:\n",
    "        # SQLALCHEMY_URL = f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{SQL_DATABASE_NAME}\"\n",
    "        get_secret_value_response = get_secret(\"SQLALCHEMY_URL\")\n",
    "        SQLALCHEMY_URL = get_secret_value_response['SecretString']\n",
    "        \n",
    "        engine = create_engine(SQLALCHEMY_URL)\n",
    "        with engine.connect() as connection:\n",
    "            result = connection.execute(text(statement))\n",
    "            return result.fetchall()\n",
    "    except Exception as e:\n",
    "        error = f\"Error executing statement: {e}\"\n",
    "        raise\n",
    "\n",
    "\n",
    "def get_database_details():\n",
    "\n",
    "    sql_query = \"\"\"\n",
    "        WITH table_columns AS (\n",
    "        SELECT \n",
    "            table_schema,\n",
    "            table_name,\n",
    "            string_agg(column_name || ' ' || data_type || \n",
    "                    CASE \n",
    "                        WHEN character_maximum_length IS NOT NULL THEN '(' || character_maximum_length || ')'\n",
    "                        ELSE ''\n",
    "                    END || \n",
    "                    CASE \n",
    "                        WHEN is_nullable = 'NO' THEN ' NOT NULL'\n",
    "                        ELSE ''\n",
    "                    END,\n",
    "                    ', ' ORDER BY ordinal_position) AS table_definition\n",
    "        FROM \n",
    "            information_schema.columns\n",
    "        WHERE \n",
    "            table_schema NOT IN ('pg_catalog', 'information_schema')\n",
    "        GROUP BY \n",
    "            table_schema, table_name\n",
    "    )\n",
    "    SELECT \n",
    "        tc.table_schema,\n",
    "        tc.table_name,\n",
    "        tc.table_definition\n",
    "    FROM \n",
    "        table_columns tc\n",
    "    ORDER BY \n",
    "        tc.table_schema, \n",
    "        tc.table_name;\n",
    "        \"\"\"\n",
    "\n",
    "    table_details = run_sql_query(sql_query)\n",
    "    table_summaries = []\n",
    "    bedrock_client = boto3.client('bedrock-runtime', region_name=REGION)\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    for table in table_details:\n",
    "        table_schema, table_name, table_definition = table\n",
    "\n",
    "        # get sample values\n",
    "        sample_values = run_sql_query(f\"SELECT * FROM {table_schema}.{table_name} LIMIT 5;\")\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        You are a helpful assistant that summarizes a Postgres table following the instructions below.\n",
    "        - The summary should be a short description of the table and what questions it might be used to answer.\n",
    "        - The summary should include the table name, table columns, and sample values.\n",
    "        - The summary should be in plain text.  \n",
    "\n",
    "        Here are the details of the table:\n",
    "        Table Name: {table_name}\n",
    "        Table Columns: {table_definition}\n",
    "        Sample Values: {sample_values}\n",
    "        \"\"\"\n",
    "\n",
    "        body = json.dumps({\n",
    "            \"inputText\": f\"\\n\\nHuman: {prompt}\\n\\nAssistant:\",\n",
    "            \"textGenerationConfig\": {\n",
    "                \"maxTokenCount\": 512,\n",
    "                \"temperature\": 0.5,\n",
    "            },\n",
    "        })\n",
    "\n",
    "        bedrock_response = bedrock_client.invoke_model(body=body, modelId=\"amazon.titan-text-premier-v1:0\")\n",
    "        response_body = json.loads(bedrock_response.get('body').read())\n",
    "\n",
    "        Text = response_body[\"results\"][0][\"outputText\"]\n",
    "\n",
    "        docID = f\"{table_schema}.{table_name}\"\n",
    "        Title = table_name\n",
    "        CrawledDate = datetime.now().isoformat()\n",
    "\n",
    "        # create a html file for each table and store it in s3\n",
    "        html = f\"<html><body><h1>{docID} - CrawledDate: {CrawledDate}</h1><pre>{Text}</pre></body></html>\"\n",
    "\n",
    "        s3_client.put_object(Bucket=DEMO_S3_BUCKET, Key=f\"{DEMO_S3_KEY}{table_schema}.{table_name}.html\", Body=html)\n",
    "\n",
    "        Url = f\"{CLOUDFRONT_URL}/{DEMO_S3_KEY}{table_schema}.{table_name}.html\"\n",
    "\n",
    "        table_summaries.append({\n",
    "            \"docID\": docID,\n",
    "            \"Title\": Title,\n",
    "            \"Text\": Text,\n",
    "            \"Url\": Url,\n",
    "            \"CrawledDate\": CrawledDate,\n",
    "            \"CreatedBy\": \"huthmac@amazon.com\"\n",
    "        })\n",
    "    # convert to dataframe\n",
    "    table_summaries_df = pd.DataFrame(table_summaries)\n",
    "\n",
    "    return table_summaries_df\n",
    "\n",
    "table_summaries_df = get_database_details()\n",
    "table_summaries_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start data source sync operation: \n",
      "{'ExecutionId': '8d9e23cd-1a08-4e2e-a017-d40c7a9a6e54', 'ResponseMetadata': {'RequestId': '3163a166-18de-46cc-9e6d-2e52fd1321fe', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '3163a166-18de-46cc-9e6d-2e52fd1321fe', 'content-type': 'application/x-amz-json-1.1', 'content-length': '54', 'date': 'Fri, 11 Oct 2024 20:04:57 GMT'}, 'RetryAttempts': 0}}\n",
      "Job execution ID: 8d9e23cd-1a08-4e2e-a017-d40c7a9a6e54\n",
      "{'ResponseMetadata': {'RequestId': 'c39ae552-34b5-4a97-8d67-bb7fe76e7aae', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'c39ae552-34b5-4a97-8d67-bb7fe76e7aae', 'content-type': 'application/x-amz-json-1.1', 'content-length': '0', 'date': 'Fri, 11 Oct 2024 20:04:57 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Synchronize custom data source 1\n",
    "# iterate through the table_summaries_df in batches of 10\n",
    "batch = table_summaries_df.iloc[:10]\n",
    "result = run_data_source_sync_job(CUSTOM_DATA_SOURCE_ID_1, KENDRA_INDEX, batch)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source created with ID: db538731-c786-4711-a5d3-54dc9f57352e\n",
      "custom_data_source_id_2: db538731-c786-4711-a5d3-54dc9f57352e\n"
     ]
    }
   ],
   "source": [
    "# CREATE CUSTOM DATA SOURCE 2\n",
    "if CUSTOM_DATA_SOURCE_ID_2 == \"XXX\":\n",
    "    custom_data_source_id_2 = create_custom_data_source(KENDRA_INDEX, \n",
    "                                                \"custom-AWSGlue\",\n",
    "                                                \"Custom data source for AWS Glue Jobs\"\n",
    "                                                )\n",
    "\n",
    "    print(f'custom_data_source_id_2: {custom_data_source_id_2}')\n",
    "    os.environ['CUSTOM_DATA_SOURCE_ID_2'] = custom_data_source_id_2\n",
    "    dotenv.set_key(local_env_filename, \"CUSTOM_DATA_SOURCE_ID_2\", os.environ[\"CUSTOM_DATA_SOURCE_ID_2\"])\n",
    "    CUSTOM_DATA_SOURCE_ID_2=os.environ[\"CUSTOM_DATA_SOURCE_ID_2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://aws-glue-assets-026459568683-us-east-1/scripts/BQ-ETL.py\n",
      "s3://aws-glue-assets-026459568683-us-east-1/scripts/GlueUpdateTableColumns.py\n",
      "s3://aws-glue-assets-026459568683-us-east-1/scripts/PostGresExport.py\n",
      "processing job: BQ-ETL\n",
      "processing job: GlueUpdateTableColumns\n",
      "processing job: PostGresExport\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Url</th>\n",
       "      <th>CrawledDate</th>\n",
       "      <th>CreatedBy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BQ-ETL</td>\n",
       "      <td>BQ-ETL</td>\n",
       "      <td>The AWS Glue ETL job BQ-ETL was run 5 times in...</td>\n",
       "      <td>https://d3q8adh3y5sxpk.cloudfront.net/rag-demo...</td>\n",
       "      <td>2024-10-11T16:05:05.449783</td>\n",
       "      <td>fhuthmacher@gmail.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GlueUpdateTableColumns</td>\n",
       "      <td>GlueUpdateTableColumns</td>\n",
       "      <td>The Glue job GlueUpdateTableColumns was last r...</td>\n",
       "      <td>https://d3q8adh3y5sxpk.cloudfront.net/rag-demo...</td>\n",
       "      <td>2024-10-11T16:05:11.444746</td>\n",
       "      <td>fhuthmacher@gmail.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PostGresExport</td>\n",
       "      <td>PostGresExport</td>\n",
       "      <td>The AWS Glue ETL job \"PostGresExport\" was last...</td>\n",
       "      <td>https://d3q8adh3y5sxpk.cloudfront.net/rag-demo...</td>\n",
       "      <td>2024-10-11T16:05:13.554312</td>\n",
       "      <td>fhuthmacher@gmail.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    docID                   Title  \\\n",
       "0                  BQ-ETL                  BQ-ETL   \n",
       "1  GlueUpdateTableColumns  GlueUpdateTableColumns   \n",
       "2          PostGresExport          PostGresExport   \n",
       "\n",
       "                                                Text  \\\n",
       "0  The AWS Glue ETL job BQ-ETL was run 5 times in...   \n",
       "1  The Glue job GlueUpdateTableColumns was last r...   \n",
       "2  The AWS Glue ETL job \"PostGresExport\" was last...   \n",
       "\n",
       "                                                 Url  \\\n",
       "0  https://d3q8adh3y5sxpk.cloudfront.net/rag-demo...   \n",
       "1  https://d3q8adh3y5sxpk.cloudfront.net/rag-demo...   \n",
       "2  https://d3q8adh3y5sxpk.cloudfront.net/rag-demo...   \n",
       "\n",
       "                  CrawledDate              CreatedBy  \n",
       "0  2024-10-11T16:05:05.449783  fhuthmacher@gmail.com  \n",
       "1  2024-10-11T16:05:11.444746  fhuthmacher@gmail.com  \n",
       "2  2024-10-11T16:05:13.554312  fhuthmacher@gmail.com  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get AWS Glue Job details\n",
    "\n",
    "# create a function that reads all AWS Glue jobs and extracts the job name, job schedules, job run history, and job script\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "BUCKET_NAME = 'felixh-kendra-demo'\n",
    "KEY_NAME = 'glue-etl-jobs/'\n",
    "\n",
    "def extract_glue_job_info(glue_client, s3_client):\n",
    "    jobs = glue_client.list_jobs()\n",
    "    job_info = []\n",
    "\n",
    "    for job_name in jobs['JobNames']:\n",
    "        job_details = glue_client.get_job(JobName=job_name)['Job']\n",
    "        job_runs = glue_client.get_job_runs(JobName=job_name)['JobRuns']\n",
    "\n",
    "        script_location = job_details.get('Command', {}).get('ScriptLocation', '')\n",
    "        print(script_location)\n",
    "        script = ''\n",
    "        if script_location:\n",
    "            # Parse the S3 URL more robustly\n",
    "            from urllib.parse import urlparse\n",
    "            parsed_url = urlparse(script_location)\n",
    "            bucket_name = parsed_url.netloc\n",
    "            key = parsed_url.path.lstrip('/')\n",
    "            \n",
    "            if bucket_name and key:\n",
    "                try:\n",
    "                    s3_client.download_file(bucket_name, key, 'script.py')\n",
    "                    with open('script.py', 'r') as file:\n",
    "                        script = file.read()\n",
    "                    os.remove('script.py')\n",
    "                except Exception as e:\n",
    "                    print(f\"Error downloading script: {e}\")\n",
    "                    script = f\"Error: Unable to download script from {script_location}\"\n",
    "\n",
    "        job_info.append({\n",
    "            'JobName': job_details['Name'],\n",
    "            'JobDescription': job_details.get('Description', ''),\n",
    "            'JobSchedule': job_details.get('Schedule', ''),\n",
    "            'JobRuns': [\n",
    "                {\n",
    "                    'Id': run['Id'],\n",
    "                    'StartedOn': run['StartedOn'].isoformat(),\n",
    "                    'CompletedOn': run['CompletedOn'].isoformat() if 'CompletedOn' in run else None,\n",
    "                    'JobRunState': run['JobRunState']\n",
    "                } for run in job_runs\n",
    "            ],\n",
    "            'ScriptLocation': script_location,\n",
    "            'Script': script\n",
    "\n",
    "        })\n",
    "\n",
    "    return job_info\n",
    "\n",
    "def get_glue_job_info():\n",
    "    glue_client = boto3.client('glue', region_name=REGION)\n",
    "    bedrock_client = boto3.client('bedrock-runtime', region_name=REGION)\n",
    "    s3_client = boto3.client('s3')\n",
    "    job_info = extract_glue_job_info(glue_client, s3_client)\n",
    "\n",
    "    #iterate through job_info and create a json file for each job\n",
    "    job_summaries = []\n",
    "    for job in job_info:\n",
    "        print(f\"processing job: {job['JobName']}\")\n",
    "\n",
    "        # call amazon bedrock to create a job summary for the job\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        You are a helpful assistant that summarizes a AWS Glue ETL job following the instructions below.\n",
    "        - The summary should be a short description of the job and the script.\n",
    "        - The summary should include the job name, how often the job was run, and the job schedule if there is one.\n",
    "        - The summary should be in plain text.  \n",
    "\n",
    "        Here are the details of the Glue Job:\n",
    "        Job Name: {job['JobName']}\n",
    "        Job Description: {job['JobDescription']}\n",
    "        Job Schedule: {job['JobSchedule']}\n",
    "        Job Runs: {job['JobRuns']}\n",
    "        Script Location: {job['ScriptLocation']}\n",
    "        Script: {job['Script']}\n",
    "        \"\"\"\n",
    "\n",
    "        body = json.dumps({\n",
    "            \"inputText\": f\"\\n\\nHuman: {prompt}\\n\\nAssistant:\",\n",
    "            \"textGenerationConfig\": {\n",
    "                \"maxTokenCount\": 512,\n",
    "                \"temperature\": 0.5,\n",
    "            },\n",
    "        })\n",
    "\n",
    "        \n",
    "        bedrock_response = bedrock_client.invoke_model(body=body, modelId=\"amazon.titan-text-premier-v1:0\")\n",
    "        response_body = json.loads(bedrock_response.get('body').read())\n",
    "\n",
    "        Text = response_body[\"results\"][0][\"outputText\"]\n",
    "        Title = job['JobName']\n",
    "        Url =  job['ScriptLocation']\n",
    "        # get current date and time\n",
    "        CrawledDate = datetime.now().isoformat()\n",
    "        docID =  job['JobName']\n",
    "\n",
    "        # create a html file for each table and store it in s3\n",
    "        html = f\"<html><body><h1>{docID} - CrawledDate: {CrawledDate}</h1><pre>{Text}</pre></body></html>\"\n",
    "\n",
    "        s3_client.put_object(Bucket=DEMO_S3_BUCKET, Key=f\"{DEMO_S3_KEY}{docID}.html\", Body=html)\n",
    "\n",
    "        Url = f\"{CLOUDFRONT_URL}/{DEMO_S3_KEY}{docID}.html\"\n",
    "\n",
    "        job_summaries.append({\n",
    "            \"docID\": docID,\n",
    "            \"Title\": Title,\n",
    "            \"Text\": Text,\n",
    "            \"Url\": Url,\n",
    "            \"CrawledDate\": CrawledDate,\n",
    "            \"CreatedBy\": \"fhuthmacher@gmail.com\"\n",
    "        })\n",
    "\n",
    "    # convert to dataframe\n",
    "    df = pd.DataFrame(job_summaries)\n",
    "    return df\n",
    "\n",
    "job_summaries_df = get_glue_job_info()\n",
    "job_summaries_df.head()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start data source sync operation: \n",
      "{'ExecutionId': 'fa2426cf-3a60-461a-8157-8052656a8e57', 'ResponseMetadata': {'RequestId': 'b534c41a-f8fa-460b-bf75-2731360e5edc', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'b534c41a-f8fa-460b-bf75-2731360e5edc', 'content-type': 'application/x-amz-json-1.1', 'content-length': '54', 'date': 'Fri, 11 Oct 2024 20:05:13 GMT'}, 'RetryAttempts': 0}}\n",
      "Job execution ID: fa2426cf-3a60-461a-8157-8052656a8e57\n",
      "{'ResponseMetadata': {'RequestId': '3be6b9c0-3be9-4bf5-86be-05318656312d', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '3be6b9c0-3be9-4bf5-86be-05318656312d', 'content-type': 'application/x-amz-json-1.1', 'content-length': '0', 'date': 'Fri, 11 Oct 2024 20:05:14 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Synchronize custom data source 2\n",
    "\n",
    "result = run_data_source_sync_job(CUSTOM_DATA_SOURCE_ID_2, KENDRA_INDEX, job_summaries_df)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Q custom data sources and sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper method to create a Q custom data source\n",
    "\n",
    "import boto3\n",
    "def create_q_custom_data_source(displayName):\n",
    "    amazonq_client = boto3.client('qbusiness', region_name=REGION)\n",
    "    response = amazonq_client.list_indices(\n",
    "        applicationId=AMAZON_Q_APP_ID,\n",
    "        \n",
    "    )\n",
    "    indexId = response['indices'][0]['indexId']\n",
    "    configuration = {\n",
    "        \"type\": \"CUSTOM\"\n",
    "    }\n",
    "    \n",
    "    response = amazonq_client.create_data_source(\n",
    "        applicationId=AMAZON_Q_APP_ID,\n",
    "        indexId=indexId,\n",
    "        displayName=displayName,\n",
    "        configuration=configuration)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper method to parse the dataframe to documents for Q ingestion\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "def parse_to_q_docs(dataSourceId, jobExecutionId, df):\n",
    "    documents = []\n",
    "   \n",
    "    for index_label, row_series in df.iterrows():\n",
    "        Text = df.at[index_label , 'Text']\n",
    "        Title = df.at[index_label , 'Title']\n",
    "        Url =  df.at[index_label , 'Url']\n",
    "        CrawledDate = df.at[index_label , 'CrawledDate']\n",
    "        docID =  df.at[index_label , 'docID']\n",
    "        CreatedBy = df.at[index_label , 'CreatedBy']\n",
    "        print(f\"docID: {docID}\")\n",
    "        doc = {\n",
    "            \"id\": docID,\n",
    "            \"content\":{\n",
    "                \"blob\": Text,\n",
    "            },\n",
    "            \"contentType\": \"PLAIN_TEXT\",\n",
    "            \"title\": Title,\n",
    "            \"attributes\": [\n",
    "                {\n",
    "                \"name\": \"_data_source_id\",\n",
    "                \"value\": {\n",
    "                    \"stringValue\": dataSourceId\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                \"name\": \"_data_source_sync_job_execution_id\",\n",
    "                \"value\": {\n",
    "                    \"stringValue\": jobExecutionId\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                \"name\": \"_source_uri\",\n",
    "                \"value\": {\n",
    "                    \"stringValue\": Url\n",
    "                    }    \n",
    "                },\n",
    "                {\n",
    "                \"name\": \"_created_at\",\n",
    "                \"value\": {\n",
    "                    \"dateValue\": CrawledDate\n",
    "                    }    \n",
    "                },\n",
    "                \n",
    "            ],\n",
    "            # \"accessConfiguration\": {\n",
    "            #     \"accessControls\": [\n",
    "            #         {\n",
    "            #             \"principals\": [\n",
    "            #                 {\n",
    "            #                     \"user\": {\n",
    "            #                         \"id\": \"arn:aws:iam::026459568683:user/huthmac\",\n",
    "            #                         \"access\": \"ALLOW\"\n",
    "            #                     }\n",
    "            #                 }\n",
    "            #             ]\n",
    "            #         }\n",
    "            #     ]\n",
    "            # }\n",
    "        }\n",
    "        documents.append(doc)\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the Q data source sync job\n",
    "\n",
    "def run_q_data_source_sync_job(data_source_id, df):\n",
    "    amazonq_client = boto3.client('qbusiness', region_name=REGION)\n",
    "    response = amazonq_client.list_indices(\n",
    "        applicationId=AMAZON_Q_APP_ID,\n",
    "        \n",
    "    )\n",
    "    index_id = response['indices'][0]['indexId']\n",
    "\n",
    "    #Start a data source sync job\n",
    "    result = amazonq_client.start_data_source_sync_job(\n",
    "        applicationId=AMAZON_Q_APP_ID,\n",
    "        dataSourceId = data_source_id,\n",
    "        indexId = index_id\n",
    "        )\n",
    "\n",
    "    print(\"Start data source sync operation: \")\n",
    "    print(result)\n",
    "\n",
    "    #Obtain the job execution ID from the result\n",
    "    job_execution_id = result['executionId']\n",
    "    print(\"Job execution ID: \"+job_execution_id)\n",
    "\n",
    "    #Start ingesting documents\n",
    "    try:\n",
    "        #parse docs for ingestion\n",
    "        docs = parse_to_q_docs(data_source_id, job_execution_id, df)\n",
    "        #batchput docs\n",
    "        # https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/qbusiness/client/batch_put_document.html\n",
    "        result = amazonq_client.batch_put_document(\n",
    "            applicationId=AMAZON_Q_APP_ID,\n",
    "            indexId = index_id,\n",
    "            documents = docs\n",
    "            )\n",
    "        print(f\"Batch put document result: {result}\")\n",
    "\n",
    "        #Stop data source sync\n",
    "        result = amazonq_client.stop_data_source_sync_job(\n",
    "            applicationId=AMAZON_Q_APP_ID,\n",
    "            dataSourceId = data_source_id,\n",
    "            indexId = index_id\n",
    "            )\n",
    "        print(f\"Stop data source sync result: {result}\")\n",
    "\n",
    "    #Stop data source sync job\n",
    "    finally:\n",
    "        #Stop data source sync\n",
    "        result = amazonq_client.stop_data_source_sync_job(\n",
    "            applicationId=AMAZON_Q_APP_ID,\n",
    "            dataSourceId = data_source_id,\n",
    "            indexId = index_id\n",
    "            )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_CUSTOM_DATA_SOURCE_ID_1: e3a4cebe-c885-466b-b201-11ded3f7913f\n"
     ]
    }
   ],
   "source": [
    "# create Q custom data source\n",
    "\n",
    "if Q_CUSTOM_DATA_SOURCE_ID_1 == \"XXX\":\n",
    "    displayName = 'custom-glue-jobs-data-source'\n",
    "    response = create_q_custom_data_source(displayName)\n",
    "    Q_CUSTOM_DATA_SOURCE_ID_1 = response['dataSourceId']\n",
    "    print(f\"Q_CUSTOM_DATA_SOURCE_ID_1: {Q_CUSTOM_DATA_SOURCE_ID_1}\")\n",
    "    os.environ['Q_CUSTOM_DATA_SOURCE_ID_1'] = Q_CUSTOM_DATA_SOURCE_ID_1\n",
    "    dotenv.set_key(local_env_filename, \"Q_CUSTOM_DATA_SOURCE_ID_1\", os.environ[\"Q_CUSTOM_DATA_SOURCE_ID_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start data source sync operation: \n",
      "{'ResponseMetadata': {'RequestId': '1184b3d7-2205-4f23-ac7f-373fcbb7d31c', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '1184b3d7-2205-4f23-ac7f-373fcbb7d31c', 'strict-transport-security': 'max-age=47304000; includeSubDomains', 'cache-control': 'no-store, no-cache, no-cache', 'date': 'Fri, 11 Oct 2024 20:37:00 GMT', 'content-type': 'application/json', 'content-length': '54', 'connection': 'keep-alive'}, 'RetryAttempts': 0}, 'executionId': '45d0a001-79e6-43cc-8789-46a4b19f65d7'}\n",
      "Job execution ID: 45d0a001-79e6-43cc-8789-46a4b19f65d7\n",
      "docID: BQ-ETL\n",
      "docID: GlueUpdateTableColumns\n",
      "docID: PostGresExport\n",
      "Batch put document result: {'ResponseMetadata': {'RequestId': 'd88e01bb-4a57-4898-86c4-bf7458a98450', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'd88e01bb-4a57-4898-86c4-bf7458a98450', 'strict-transport-security': 'max-age=47304000; includeSubDomains', 'cache-control': 'no-store, no-cache, no-cache', 'date': 'Fri, 11 Oct 2024 20:37:00 GMT', 'content-type': 'application/json', 'content-length': '22', 'connection': 'keep-alive'}, 'RetryAttempts': 0}, 'failedDocuments': []}\n",
      "Stop data source sync result: {'ResponseMetadata': {'RequestId': '72a76579-0875-4477-bace-9ff4a2f3ede3', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '72a76579-0875-4477-bace-9ff4a2f3ede3', 'strict-transport-security': 'max-age=47304000; includeSubDomains', 'cache-control': 'no-store, no-cache, no-cache', 'date': 'Fri, 11 Oct 2024 20:37:01 GMT', 'content-type': 'application/json', 'content-length': '2', 'connection': 'keep-alive'}, 'RetryAttempts': 0}}\n",
      "{'ResponseMetadata': {'RequestId': '580c0235-0f7e-4e2c-ac95-aae56d0fb0d0', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '580c0235-0f7e-4e2c-ac95-aae56d0fb0d0', 'strict-transport-security': 'max-age=47304000; includeSubDomains', 'cache-control': 'no-store, no-cache, no-cache', 'date': 'Fri, 11 Oct 2024 20:37:02 GMT', 'content-type': 'application/json', 'content-length': '2', 'connection': 'keep-alive'}, 'RetryAttempts': 1}}\n"
     ]
    }
   ],
   "source": [
    "# Sync custom glue data source\n",
    "result = run_q_data_source_sync_job(Q_CUSTOM_DATA_SOURCE_ID_1, job_summaries_df)\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_CUSTOM_DATA_SOURCE_ID_2: 7e6bd98b-081a-4de9-a9af-ad296d9374cc\n"
     ]
    }
   ],
   "source": [
    "if Q_CUSTOM_DATA_SOURCE_ID_2 == \"XXX\":\n",
    "    displayName = 'custom-postgres-data-source'\n",
    "    response = create_q_custom_data_source(displayName)\n",
    "    Q_CUSTOM_DATA_SOURCE_ID_2 = response['dataSourceId']\n",
    "    print(f\"Q_CUSTOM_DATA_SOURCE_ID_2: {Q_CUSTOM_DATA_SOURCE_ID_2}\")\n",
    "    os.environ['Q_CUSTOM_DATA_SOURCE_ID_2'] = Q_CUSTOM_DATA_SOURCE_ID_2\n",
    "    dotenv.set_key(local_env_filename, \"Q_CUSTOM_DATA_SOURCE_ID_2\", os.environ[\"Q_CUSTOM_DATA_SOURCE_ID_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start data source sync operation: \n",
      "{'ResponseMetadata': {'RequestId': 'f746910d-e9f4-47e4-8855-0b0b6a6fa376', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'f746910d-e9f4-47e4-8855-0b0b6a6fa376', 'strict-transport-security': 'max-age=47304000; includeSubDomains', 'cache-control': 'no-store, no-cache, no-cache', 'date': 'Fri, 11 Oct 2024 20:37:54 GMT', 'content-type': 'application/json', 'content-length': '54', 'connection': 'keep-alive'}, 'RetryAttempts': 0}, 'executionId': '0f0e1afa-675d-467d-86b9-cf1e8fe2cb7a'}\n",
      "Job execution ID: 0f0e1afa-675d-467d-86b9-cf1e8fe2cb7a\n",
      "docID: public.categories\n",
      "docID: public.customer_customer_demo\n",
      "docID: public.customer_demographics\n",
      "docID: public.customers\n",
      "docID: public.employee_territories\n",
      "docID: public.employees\n",
      "docID: public.order_details\n",
      "docID: public.orders\n",
      "docID: public.productreviews\n",
      "docID: public.products\n",
      "Batch put document result: {'ResponseMetadata': {'RequestId': '464cb51b-448e-4df9-b794-954dc787be33', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '464cb51b-448e-4df9-b794-954dc787be33', 'strict-transport-security': 'max-age=47304000; includeSubDomains', 'cache-control': 'no-store, no-cache, no-cache', 'date': 'Fri, 11 Oct 2024 20:37:56 GMT', 'content-type': 'application/json', 'content-length': '22', 'connection': 'keep-alive'}, 'RetryAttempts': 0}, 'failedDocuments': []}\n",
      "Stop data source sync result: {'ResponseMetadata': {'RequestId': 'b34af56c-7efc-4b06-bd9e-e6f177a74657', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'b34af56c-7efc-4b06-bd9e-e6f177a74657', 'strict-transport-security': 'max-age=47304000; includeSubDomains', 'cache-control': 'no-store, no-cache, no-cache', 'date': 'Fri, 11 Oct 2024 20:37:57 GMT', 'content-type': 'application/json', 'content-length': '2', 'connection': 'keep-alive'}, 'RetryAttempts': 0}}\n",
      "{'ResponseMetadata': {'RequestId': '869442ca-b016-4996-b12c-81a1badc499d', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '869442ca-b016-4996-b12c-81a1badc499d', 'strict-transport-security': 'max-age=47304000; includeSubDomains', 'cache-control': 'no-store, no-cache, no-cache', 'date': 'Fri, 11 Oct 2024 20:37:58 GMT', 'content-type': 'application/json', 'content-length': '2', 'connection': 'keep-alive'}, 'RetryAttempts': 1}}\n"
     ]
    }
   ],
   "source": [
    "# Sync custom postgres data source\n",
    "result = run_q_data_source_sync_job(Q_CUSTOM_DATA_SOURCE_ID_2, table_summaries_df[:10])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create some sample data for standard RDS Postgres database Connector\n",
    "\n",
    "# def create_sql(statement):\n",
    "#     try:\n",
    "#             # SQLALCHEMY_URL = f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{SQL_DATABASE_NAME}\"\n",
    "#         get_secret_value_response = get_secret(\"SQLALCHEMY_URL\")\n",
    "#         SQLALCHEMY_URL = get_secret_value_response['SecretString']\n",
    "        \n",
    "#         engine = create_engine(SQLALCHEMY_URL)\n",
    "#         with engine.connect() as connection:\n",
    "#             connection.execute(text(statement))\n",
    "#             connection.commit()\n",
    "#     except Exception as e:\n",
    "#         error = f\"Error executing statement: {e}\"\n",
    "#         raise\n",
    "\n",
    "# create_sql(\"\"\"\n",
    "# CREATE TABLE public.ProductReviews (\n",
    "#     ProductID INT,\n",
    "#     ProductName VARCHAR(100),\n",
    "#     Review TEXT,\n",
    "#     Rating INT,\n",
    "#     Reviewer VARCHAR(100),\n",
    "#     CreatedBy VARCHAR(100)\n",
    "# );\n",
    "# \"\"\")\n",
    "\n",
    "# create_sql(\"\"\"\n",
    "# INSERT INTO public.ProductReviews (ProductID, ProductName, Review, Rating, Reviewer, CreatedBy)\n",
    "# VALUES \n",
    "# (1, 'Wireless Earbuds', 'Great sound quality and comfortable fit!', 5, 'John Smith', 'huthmac@amazon.com'),\n",
    "# (2, 'Smart Watch', 'Decent features but battery life could be better.', 4, 'Emma Johnson', 'huthmac@amazon.com'),\n",
    "# (3, 'Laptop', 'Excellent performance for the price.', 5, 'Michael Brown', 'huthmac@amazon.com'),\n",
    "# (4, 'Coffee Maker', 'Makes great coffee but a bit noisy.', 4, 'Sarah Davis', 'huthmac@amazon.com'),\n",
    "# (5, 'Fitness Tracker', 'Accurate step counting, but the app needs improvement.', 3, 'David Wilson', 'huthmac@amazon.com'),\n",
    "# (6, 'Bluetooth Speaker', 'Impressive sound for its size!', 5, 'Lisa Anderson', 'huthmac@amazon.com'),\n",
    "# (7, 'Electric Toothbrush', 'My teeth feel cleaner, but it''s a bit pricey.', 4, 'Robert Taylor', 'huthmac@amazon.com'),\n",
    "# (8, 'Air Fryer', 'Cooks food quickly and evenly. Easy to clean too!', 5, 'Jennifer Martinez', 'huthmac@amazon.com'),\n",
    "# (9, 'Gaming Mouse', 'Responsive and comfortable for long gaming sessions.', 5, 'Chris Lee', 'huthmac@amazon.com'),\n",
    "# (10, 'Portable Charger', 'Charges quickly but doesn''t hold as much power as advertised.', 3, 'Emily White', 'huthmac@amazon.com');\n",
    "# \"\"\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
