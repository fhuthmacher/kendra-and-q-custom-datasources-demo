{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create an Amazon Kendra S3 data source with access control list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and load environment variables\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv, set_key\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "# loading environment variables that are stored in local file\n",
    "local_env_filename = 'dev.env'\n",
    "load_dotenv(find_dotenv(local_env_filename),override=True)\n",
    "\n",
    "os.environ['REGION'] = os.getenv('REGION')\n",
    "os.environ['KENDRA_INDEX'] = os.getenv('KENDRA_INDEX')\n",
    "os.environ['KENDRA_ROLE'] = os.getenv('KENDRA_ROLE')\n",
    "os.environ['CUSTOM_DATA_SOURCE_ID_1'] = os.getenv('CUSTOM_DATA_SOURCE_ID_1')\n",
    "os.environ['CUSTOM_DATA_SOURCE_ID_2'] = os.getenv('CUSTOM_DATA_SOURCE_ID_2')\n",
    "os.environ['AMAZON_Q_APP_ID'] = os.getenv('AMAZON_Q_APP_ID')\n",
    "os.environ['Q_CUSTOM_DATA_SOURCE_ID_1'] = os.getenv('Q_CUSTOM_DATA_SOURCE_ID_1')\n",
    "os.environ['Q_CUSTOM_DATA_SOURCE_ID_2'] = os.getenv('Q_CUSTOM_DATA_SOURCE_ID_2')\n",
    "os.environ['DEMO_S3_BUCKET'] = os.getenv('DEMO_S3_BUCKET')\n",
    "os.environ['DEMO_S3_KEY'] = os.getenv('DEMO_S3_KEY')\n",
    "os.environ['CLOUDFRONT_URL'] = os.getenv('CLOUDFRONT_URL')\n",
    "\n",
    "REGION = os.environ['REGION']\n",
    "KENDRA_INDEX = os.environ['KENDRA_INDEX']\n",
    "KENDRA_ROLE = os.environ['KENDRA_ROLE']\n",
    "CUSTOM_DATA_SOURCE_ID_1 = os.environ['CUSTOM_DATA_SOURCE_ID_1']\n",
    "CUSTOM_DATA_SOURCE_ID_2 = os.environ['CUSTOM_DATA_SOURCE_ID_2']\n",
    "AMAZON_Q_APP_ID = os.environ['AMAZON_Q_APP_ID']\n",
    "Q_CUSTOM_DATA_SOURCE_ID_1 = os.environ['Q_CUSTOM_DATA_SOURCE_ID_1']\n",
    "Q_CUSTOM_DATA_SOURCE_ID_2 = os.environ['Q_CUSTOM_DATA_SOURCE_ID_2']\n",
    "DEMO_S3_BUCKET = os.environ['DEMO_S3_BUCKET']\n",
    "DEMO_S3_KEY = os.environ['DEMO_S3_KEY']\n",
    "CLOUDFRONT_URL = os.environ['CLOUDFRONT_URL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Kendra Amazon S3 data source with ACL\n",
    "# see also https://aws.amazon.com/blogs/machine-learning/secure-your-amazon-kendra-indexes-with-the-acl-using-a-jwt-shared-secret-key/\n",
    "# and https://aws.amazon.com/blogs/machine-learning/building-a-secure-search-application-with-access-controls-using-amazon-kendra/\n",
    "# and https://docs.aws.amazon.com/kendra/latest/dg/create-index-access-control.html\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "\n",
    "def create_acl_file(bucket_name, bucket_key):\n",
    "    s3_client = boto3.client('s3')\n",
    "    acldata = [{\n",
    "        \"keyPrefix\": f\"s3://{bucket_name}/{bucket_key}\",\n",
    "        \"aclEntries\": [\n",
    "            {\n",
    "                \"Name\": \"SA\",\n",
    "                \"Type\": \"GROUP\",\n",
    "                \"Access\": \"ALLOW\"\n",
    "            }]\n",
    "    }]\n",
    "    metadata_file_name = f\"{bucket_key}acl.json\"\n",
    "    s3_client.put_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=metadata_file_name,\n",
    "        Body=json.dumps(acldata),\n",
    "        ContentType='application/json'\n",
    "    )\n",
    "\n",
    "def create_metadata_files(bucket_name, bucket_key, meta_folder='meta'):\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    # List objects in the specified bucket and prefix\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    pages = paginator.paginate(Bucket=bucket_name, Prefix=bucket_key)\n",
    "\n",
    "    for page in pages:\n",
    "        for obj in page.get('Contents', []):\n",
    "            if obj['Key'].endswith(('.txt', '.pdf', '.doc', '.docx')):  # Add or remove file types as needed\n",
    "                # Generate metadata for each file\n",
    "                metadata = {\n",
    "                    \"Attributes\": {\n",
    "                        \"DocumentType\": \"llm-papers\"  # You can customize this based on your needs\n",
    "                    },\n",
    "                    \"AccessControlList\": [\n",
    "                        { \"Access\": \"DENY\", \"Name\": \"huthmac@amazon.com\", \"Type\": \"USER\" }\n",
    "                        # Add more access control entries as needed\n",
    "                    ]\n",
    "                }\n",
    "\n",
    "                # Create metadata file name\n",
    "                metadata_file_name = f\"{meta_folder}/{obj['Key']}.json\"\n",
    "\n",
    "                # Upload metadata file to S3\n",
    "                s3_client.put_object(\n",
    "                    Bucket=bucket_name,\n",
    "                    Key=metadata_file_name,\n",
    "                    Body=json.dumps(metadata),\n",
    "                    ContentType='application/json'\n",
    "                )\n",
    "\n",
    "                print(f\"Created metadata file: {metadata_file_name}\")\n",
    "\n",
    "def create_s3_data_source(index_id, data_source_name, bucket_name, bucket_key, iam_role_arn):\n",
    "    kendra = boto3.client('kendra', region_name=REGION)\n",
    "\n",
    "    # # Create metadata files\n",
    "    # create_metadata_files(bucket_name, bucket_key)\n",
    "\n",
    "    # create ACL file\n",
    "    create_acl_file(bucket_name, bucket_key)\n",
    "\n",
    "    # S3 data source configuration\n",
    "    data_source_configuration = {\n",
    "        \"S3Configuration\": {\n",
    "            \"AccessControlListConfiguration\": {\n",
    "                \"KeyPath\": f\"s3://{bucket_name}/{bucket_key}acl.json\"\n",
    "            },\n",
    "            # \"DocumentsMetadataConfiguration\": {\n",
    "            #     \"S3Prefix\": \"meta/\"\n",
    "            # },\n",
    "            \"BucketName\": bucket_name,\n",
    "            \"InclusionPrefixes\": [\n",
    "                bucket_key\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = kendra.create_data_source(\n",
    "            IndexId=index_id,\n",
    "            Name=data_source_name,\n",
    "            Type='S3',\n",
    "            Configuration=data_source_configuration,\n",
    "            RoleArn=iam_role_arn,\n",
    "            LanguageCode='en',\n",
    "            Description='Amazon S3 data source for Kendra index'\n",
    "        )\n",
    "        \n",
    "        print(f\"Data source created successfully. Data source ID: {response['Id']}\")\n",
    "        return response['Id']\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating S3 data source: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Usage\n",
    "index_id = KENDRA_INDEX  # Use your existing Kendra index ID\n",
    "data_source_name = 's3-llmpaperstest'\n",
    "bucket_name = 'felixh-kendra-demo'  # Use your S3 bucket name\n",
    "bucket_key = 'llmpapers/'\n",
    "iam_role_arn = KENDRA_ROLE  # IAM role ARN with necessary permissions\n",
    "\n",
    "s3_data_source_id = create_s3_data_source(index_id, data_source_name, bucket_name, bucket_key, iam_role_arn)\n",
    "\n",
    "if s3_data_source_id:\n",
    "    print(f\"S3 data source created with ID: {s3_data_source_id}\")\n",
    "    # Save the data source ID to your environment variables\n",
    "    os.environ['S3_DATA_SOURCE_ID'] = s3_data_source_id\n",
    "    dotenv.set_key(local_env_filename, \"S3_DATA_SOURCE_ID\", os.environ[\"S3_DATA_SOURCE_ID\"])\n",
    "else:\n",
    "    print(\"Failed to create S3 data source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run Kendra Amazon S3 data source sync job\n",
    "import boto3\n",
    "import time\n",
    "\n",
    "def start_s3_sync_job(index_id, data_source_id):\n",
    "    kendra = boto3.client('kendra')\n",
    "\n",
    "    try:\n",
    "        response = kendra.start_data_source_sync_job(\n",
    "            Id=data_source_id,\n",
    "            IndexId=index_id\n",
    "        )\n",
    "        execution_id = response['ExecutionId']\n",
    "        print(f\"Sync job started. Execution ID: {execution_id}\")\n",
    "        return execution_id\n",
    "    except Exception as e:\n",
    "        print(f\"Error starting sync job: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def monitor_sync_job(index_id, data_source_id, execution_id):\n",
    "    kendra = boto3.client('kendra')\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            response = kendra.list_data_source_sync_jobs(\n",
    "                Id=data_source_id,\n",
    "                IndexId=index_id\n",
    "            )\n",
    "            \n",
    "            for job in response['History']:\n",
    "                if job['ExecutionId'] == execution_id:\n",
    "                    status = job['Status']\n",
    "                    print(f\"Sync job status: {status}\")\n",
    "                    \n",
    "                    if status in ['FAILED', 'SUCCEEDED']:\n",
    "                        if status == 'FAILED':\n",
    "                            print(f\"Sync job failed. Error message: {job.get('ErrorMessage', 'No error message provided')}\")\n",
    "                        return status\n",
    "                    \n",
    "                    break\n",
    "            \n",
    "            time.sleep(60)  # Wait for 60 seconds before checking again\n",
    "        except Exception as e:\n",
    "            print(f\"Error monitoring sync job: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "# Usage\n",
    "index_id = KENDRA_INDEX  # Use your existing Kendra index ID\n",
    "data_source_id = os.environ['S3_DATA_SOURCE_ID']  # Use the S3 data source ID we just created\n",
    "\n",
    "execution_id = start_s3_sync_job(index_id, data_source_id)\n",
    "\n",
    "if execution_id:\n",
    "    final_status = monitor_sync_job(index_id, data_source_id, execution_id)\n",
    "    if final_status == 'SUCCEEDED':\n",
    "        print(\"S3 data source sync completed successfully\")\n",
    "    else:\n",
    "        print(\"S3 data source sync failed or was interrupted\")\n",
    "else:\n",
    "    print(\"Failed to start S3 data source sync job\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
